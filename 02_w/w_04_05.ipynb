{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd062b65a3e32fd75c6613f438e3e64606454bf9518010d2b50fe61db1c37149f34",
   "display_name": "Python 3.7.9 64-bit ('Scrapping-CfF0pIyJ')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "w_04_05 | BeautifulSoup y HTML\n",
    "===="
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "BeautifulSoup es, junto con Scrapy, el paquete más usado para scrapping web. Su uso básico tiene una lógica muy simple que consiste en proveerle el código fuente de una página (el HTML) para que construya un \"árbol\" con sus elementos en donde podemos buscar lo que necesitamos.\n",
    "\n",
    "Busquemos todas las noticias de la portada de Montevideo Portal, para lo que hay que inspeccionar la fuente.\n",
    "\n",
    "<img src=\"files\\beautifulsoup_montevideo.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.montevideo.com.uy/categoria/Noticias-310\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content)\n",
    "find = soup.find_all(\"h2\", class_=\"title\")\n",
    "results = {el.contents[0].text: el.contents[0].get(\"href\") for el in find}\n",
    "pprint(results, indent=4)"
   ]
  },
  {
   "source": [
    "Hicimos lo siguiente:\n",
    "\n",
    "1. Un request para obtener el HTML.\n",
    "2. Pasamos el contenido de la respuesta a la clase ``BeautifulSoup`` (se llama igual que el paquete, pero se importa de ``bs4``), lo que devuelve un objeto que podemos procesar fácilmente.\n",
    "3. Usamos el método ``find_all()`` para que encuentre todos los elementos con el tag h2 y la class \"title\", porque eso es lo que figura cuando hacemos inspect en un browser.\n",
    "4. El texto y el link están un nivel más abajo del tag h2, por lo que tenemos que acceder a los ``contents`` (es una lista, por eso accedemos al elemento 0, el único que hay en este caso).\n",
    "5. Armamos una dict comprehension que extrae el texto y el link (href) de cada elemento encontrado.\n",
    "\n",
    "¿Por qué no buscamos directamente los tags a (que son links)? Porque necesitamos alguna forma de \"filtrar\" los resultados, y como ese tag no tiene demasiados detalles, recurrimos a su parent h2. Sino, pasa lo siguiente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")[:10]"
   ]
  },
  {
   "source": [
    "Con el método ``find_all()`` y un poco de creatividad se puede hacer prácticamente todo.\n",
    "\n",
    "La idea ahora es extraer avisos de puestos de trabajo de El Gallito. En la URL de búsqueda hay información limitada, por lo que habrá que entrar uno por uno, para lo cual necesitamos extraer las URLs.\n",
    "\n",
    "Vamos a concentranos en los resultados de la primera página.\n",
    "\n",
    "El proceso es igual, inspeccionamos el elemento que nos interesa, ubicamos los tags relevantes y extraemos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "gallito = \"https://trabajo.gallito.com.uy/buscar\"\n",
    "r = requests.get(gallito)\n",
    "soup = BeautifulSoup(r.content)\n",
    "find = soup.find_all(\"a\", class_=\"post-cuadro row smB\")\n",
    "urls = [puesto.get(\"href\") for puesto in find]\n",
    "urls"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "La tarea ahora es constuir las URLs, hacer el request y luego el scrapping.\n",
    "\n",
    "La ventaja de HTML es que suele estar todo en el mismo lugar. Entonces con ver la estructura en un puesto podemos saber cómo van a funcionar el resto."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puestos = []\n",
    "with requests.Session() as s:\n",
    "    for url in urls:\n",
    "        full_url = f\"https://trabajo.gallito.com.uy{url}\"\n",
    "        print(f\"Procesando {full_url}\")\n",
    "        r = s.get(full_url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        title = soup.find(\"div\", class_=\"title-puesto\").contents[0].text.strip()\n",
    "        subtitle = soup.find(\"div\", class_=\"subtitle-puesto\").text.strip()\n",
    "        since = soup.find(\"span\", class_=\"time-text\").text.strip()\n",
    "        cantidad = soup.find(\"div\", class_=\"span-ofertas\").text.strip()\n",
    "        titulo_textos = soup.find_all(\"div\", class_=\"cuadro-aviso-title\")\n",
    "        textos = soup.find_all(\"div\", class_=\"cuadro-aviso-text\")\n",
    "        textos = {k.text.strip(): v.text.strip() for k, v in zip(titulo_textos, textos)}\n",
    "        textos.pop(\"Avisos similares\", None)\n",
    "        data_puesto = {\"Descripción\": title, \"Empresa\": subtitle, \"Cantidad de ofertas\": cantidad, \"Fecha\": since}\n",
    "        data_puesto.update(textos)\n",
    "        puestos.append(data_puesto)\n",
    "\n",
    "df = pd.DataFrame(puestos)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Algunos comentarios:\n",
    "\n",
    "1. Usamos una ``Session`` de ``requests`` para agilizar el proceso dado que estamos entrando a varias páginas del mismo sitio.\n",
    "2. Para el título tenemos que revisar los ``contents``, porque es un h2 dentro de un div. No podemos buscar directamente los h2 porque hay otros títulos de ese tipo en la página.\n",
    "3. El subtítulo (que tiene la información de la empresa), la cantidad de ofertas y el momento de publicación no tienen dificultades, simplemente accedemos a su ``text`` y le aplicamos el método ``strip()`` para que saque todos los espacios extra.\n",
    "4. Cada puesto tiene distintos boxes con información que no están presentes en todos los avisos. Sin embargo, siempre tienen los mismos tags. Por eso  hacemos un ``find_all()`` para esos tags y luego lo procesamos con una dict comprehension.\n",
    "5. Algunos avisos tienen un box de \"Avisos similares\" que no nos interesa por ahora, así que lo eliminamos del diccionario de textos.\n",
    "6. Construimos un nuevo diccionario con el título, subtítulo, cantidad de ofertas y momento de publicación.\n",
    "7. Actualizamos este diccionario con la información de los textos.\n",
    "8. Agregamos el diccionario completo a la lista de ``puestos``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}